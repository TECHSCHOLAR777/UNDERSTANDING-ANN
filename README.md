ğŸ¤– ANN Function Approximation (Exploration)
ğŸ“˜ About this Project
This project is an exploration of how Artificial Neural Networks (ANNs) learn and approximate different mathematical functions.
ğŸ¯ The aim was not to get perfect predictions, but to understand:
- ğŸ” How neural networks behave with different types of functions
- âš¡ How activation functions affect learning
- ğŸ“ Why scaling and noise matter
- ğŸš§ Where neural networks struggle
This project was done purely as a learning exercise ğŸ§‘â€ğŸ’».

ğŸ§© Functions Explored
I experimented with different types of functions, increasing complexity step by step:
- ğŸ“ˆ Linear function â†’ y = 2x + 3
- ğŸ”„ Non-linear function â†’ y = xÂ²
- âœ‚ï¸ Non-differentiable function â†’ y = x
- â¹ï¸ Discontinuous function â†’ Step function
- ğŸ² Discrete function â†’ Floor function & XOR
- â• Multi-variable function â†’ y = xâ‚ + xâ‚‚
- ğŸŒŠ Complex function â†’ y = sin(xâ‚) + xâ‚‚Â²

ğŸ› ï¸ Dataset Creation
- ğŸ“Š Input data generated using NumPy
- ğŸ§® Outputs calculated using mathematical expressions
- ğŸ›ï¸ Small noise added to simulate real-world data
- ğŸ”„ Both input and output normalized to improve training
ğŸ’¡ Example:
X = np.linspace(-1000, 1000, 10000).reshape(-1, 1)
y = X**2

X = X / 1000
y = y / 1e6

noise = np.random.normal(0, 0.02, size=y.shape)
y_noisy = y + noise



ğŸ§  Neural Network Models
- Built using TensorFlow (Keras) âš™ï¸
- Mostly simple architectures ğŸ—ï¸
- Tried different activation functions:
- ğŸ”¥ ReLU
- ğŸŒˆ Tanh
- ğŸ“‰ Loss function: Mean Squared Error (MSE)
- ğŸš€ Optimizer: Adam
ğŸ’¡ Example model:
model = Sequential([
    Dense(32, activation='tanh', input_shape=(1,)),
    Dense(1)
])



ğŸ‘€ Observations
Some interesting things I noticed:
- âŒ Linear models fail on non-linear functions
- ğŸ“ ReLU networks approximate curves using straight-line segments
- ğŸ”€ For y = xÂ², the model sometimes learned a V-shape (x) instead of a parabola
- âœ… Normalizing both input and output greatly improved learning
- âš ï¸ Large learning rates caused unstable training
- ğŸŒªï¸ Too much noise made learning difficult

ğŸ Conclusion
This task helped me understand that:
- ğŸ§  Neural networks approximate functions rather than learning exact formulas
- ğŸ“ Data scaling and activation functions play a major role
- ğŸ—ï¸ â€œGood architectureâ€ alone does not guarantee good learning
âœ¨ Overall, this project improved my intuition about how ANNs actually work.


